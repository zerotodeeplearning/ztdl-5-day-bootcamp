{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Generation\n",
    "\n",
    "In this exercise, you will design an RNN to generate baby names! You will design an RNN to learn to predict the next letter of a name given the preceding letters. This is a character-level RNN rather than a word-level RNN.\n",
    "\n",
    "This idea comes from this excellent blog post: http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from tensorflow.keras.layers import LSTM, SimpleRNN, GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "\n",
    "The training data we will use comes from this corpus:\n",
    "http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/\n",
    "\n",
    "Take a look at the training data in `data/names.txt`, which includes both boy and girl names. Below we load the file and convert it to all lower-case for simplicity.\n",
    "\n",
    "Note that we also add a special \"end\" character (in this case a period) to allow the model to learn to predict the end of a name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/names.txt') as f:\n",
    "    names = f.readlines()\n",
    "    names = [name.lower().strip() + '.' for name in names]\n",
    "\n",
    "print('Loaded %d names' % len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to count all of the characters in our \"vocabulary\" and build a dictionary that translates between the character and its assigned index (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set()\n",
    "for name in names:\n",
    "    chars.update(name)\n",
    "vocab_size = len(chars)\n",
    "print('Vocabulary size:', vocab_size)\n",
    "\n",
    "char_inds = dict((c, i) for i, c in enumerate(chars))\n",
    "inds_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1 - translate chars to indexes\n",
    "\n",
    "Most of the work of preparing the data is taken care of, but it is important to know the steps because they will be needed anytime you want to train an RNN. Use the dictionary created above to translate each example in `names` to its number format in `int_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Translate names to their number format in int_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_matrix_from_sequences` will take the examples and create training data by cutting up names into input sequence of length `maxlen` and training labels, which are the following character. Make sure you understand this procedure because it is what will actually go into the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix_from_sequences(int_names, maxlen, step=1):\n",
    "    name_parts = []\n",
    "    next_chars = []\n",
    "    for name in int_names:\n",
    "        for i in range(0, len(name) - maxlen, step):\n",
    "            name_parts.append(name[i: i + maxlen])\n",
    "            next_chars.append(name[i + maxlen])\n",
    "\n",
    "    return name_parts, next_chars\n",
    "\n",
    "maxlen = 3\n",
    "name_parts, next_chars = create_matrix_from_sequences(int_names, maxlen)\n",
    "print('Created %d name segments' % len(name_parts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(name_parts, maxlen=maxlen)\n",
    "y_train = to_categorical(next_chars, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2 - design a model\n",
    "\n",
    "Design your model below. Like before, you will need to set up the embedding layer, the recurrent layer, a dense connection and a softmax to predict the next character.\n",
    "\n",
    "Fit the model by running at least 10 epochs. Later you will generate names with the model. Getting around 30% accuracy will usually result in decent generations. What is the accuracy you would expect for random guessing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the model\n",
    "\n",
    "We can sample the model by feeding in a few letters and using the model's prediction for the next letter. Then we feed the model's prediction back in to get the next letter, etc.\n",
    "\n",
    "The `sample` function is a helper to allow you to adjust the diversity of the samples. You can read more [here](https://en.wikipedia.org/wiki/Softmax_function#Reinforcement_learning).\n",
    "\n",
    "Read the `gen_name` function to understand how the model is sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(p, diversity=1.0):\n",
    "    p1 = np.asarray(p).astype('float64')\n",
    "    p1 = np.log(p1) / diversity\n",
    "    e_p1 = np.exp(p1)\n",
    "    s = np.sum(e_p1)\n",
    "    p1 = e_p1 / s\n",
    "    return np.argmax(np.random.multinomial(1, p1, 1))\n",
    "\n",
    "\n",
    "def gen_name(seed, length=1, diversity=1.0, maxlen=3):\n",
    "    \"\"\"\n",
    "    seed - the start of the name to sample\n",
    "    length - the number of letters to sample; if None then samples\n",
    "        are generated until the model generates a '.' character\n",
    "    diversity - a knob to increase or decrease the randomness of the\n",
    "        samples; higher = more random, lower = closer to the model's\n",
    "        prediction\n",
    "    maxlen - the size of the model's input\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare input array\n",
    "    x = np.zeros((1, maxlen), dtype=int)\n",
    "\n",
    "    # Generate samples\n",
    "    out = seed\n",
    "    while length is None or len(out) < len(seed) + length:\n",
    "\n",
    "        # Add the last chars so far for the next input\n",
    "        for i, c in enumerate(out[-maxlen:]):\n",
    "            x[0, i] = char_inds[c]\n",
    "        \n",
    "        # Get softmax for next character\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        \n",
    "        # Sample the network output with diversity\n",
    "        c = sample(preds, diversity)\n",
    "        \n",
    "        # Choose to end if the model generated an end token\n",
    "        if c == char_inds['.']:\n",
    "            if length is None:\n",
    "                return out\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        # Build up output\n",
    "        out += inds_char[c]\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3 - sample the model\n",
    "\n",
    "Use the `gen_name` function above to sample some names from your model.\n",
    "\n",
    "1. Try generating a few characters by setting the `length` argument.\n",
    "2. Try different diversities. Start with 1.0 and vary it up and down.\n",
    "3. Try using `length=None`, allowing the model to choose when to end a name.\n",
    "4. What happens when `length=None` and the diversity is high? How do samples change in this case staring from beginning to end? Why do you think this is?\n",
    "5. With `length=None` and a \"good\" diversity, can you tell if the model has learned a repertoire of \"endings\"? What are some of them? \n",
    "6. Find some good names. What are you favorites? :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4 - retrain\n",
    "\n",
    "Now that you have seen some samples, go back up and redefine your model to \"erase\" it. Don't train it again yet. You can sample again to compare the quality of the samples before the model is trained.\n",
    "\n",
    "Experiment with the hidden layer size, the maxlen, the number of epochs, etc. Do you observe any differences in the sample behavior?\n",
    "\n",
    "Not all changes will make an observable impact, but do experiments to see what you can discover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally, take a look at [this Colab notebook](https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb) for a similar example with Shakespeare text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
