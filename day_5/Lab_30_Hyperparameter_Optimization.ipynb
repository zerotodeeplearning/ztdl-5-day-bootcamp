{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats.distributions import expon, uniform, randint\n",
    "from sklearn.model_selection import train_test_split, ParameterSampler\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a couple of helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_dict(d):\n",
    "    for k, v in d.items():\n",
    "        print('  {:>20}: {}'.format(k, v))\n",
    "        \n",
    "def print_header(s):\n",
    "    divider = '=' * (len(s) + 4)\n",
    "    print()\n",
    "    print(divider)\n",
    "    print('  {}  '.format(s))\n",
    "    print(divider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(X_train_valid, y_train_valid), (X_test, y_test) = cifar10.load_data()\n",
    "X_train_valid = X_train_valid.astype('float32') / 255.\n",
    "X_test = X_test.astype('float32') / 255.\n",
    "\n",
    "y_train_valid = to_categorical(y_train_valid)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_valid, y_train_valid, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Train shapes: x = {}, y = {}'.format(\n",
    "    X_train.shape, y_train.shape))\n",
    "print('Valid shapes: x = {}, y = {}'.format(\n",
    "    X_valid.shape, y_valid.shape))\n",
    "print('Test  shapes: x = {}, y = {}'.format(\n",
    "    X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "Make a function which accepts a config object containing your hyperparameters and returns a compiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_compile(config):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # first convolution / pooling set\n",
    "    model.add(Conv2D(config.conv1_num_filters, \n",
    "                     config.conv_filter_size, \n",
    "                     activation=config.activation, \n",
    "                     padding='same',\n",
    "                     input_shape=X_train.shape[1:]))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # second convolution / pooling set\n",
    "    model.add(Conv2D(config.conv2_num_filters, \n",
    "                     config.conv_filter_size, \n",
    "                     padding='same',\n",
    "                     activation=config.activation))\n",
    "    model.add(Conv2D(config.conv3_num_filters, \n",
    "                     config.conv_filter_size, \n",
    "                     padding='same',\n",
    "                     activation=config.activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # third convolution / pooling set\n",
    "    model.add(Conv2D(config.conv4_num_filters, \n",
    "                     config.conv_filter_size, \n",
    "                     padding='same',\n",
    "                     activation=config.activation))\n",
    "    model.add(Conv2D(config.conv5_num_filters, \n",
    "                     config.conv_filter_size, \n",
    "                     padding='same',\n",
    "                     activation=config.activation))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(config.dense1_size,\n",
    "                    activation=config.activation))\n",
    "    model.add(Dropout(config.dropout))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=config.learn_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Selection\n",
    "Define the legal ranges for your hyperparameters and use `Sklearn`'s `ParameterSampler` to sample hyperparameters sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hp_ranges = {\n",
    "    'conv1_num_filters': [32, 64, 128],\n",
    "    'conv2_num_filters': [32, 64, 128],\n",
    "    'conv3_num_filters': [32, 64, 128],\n",
    "    'conv4_num_filters': [32, 64, 128],\n",
    "    'conv5_num_filters': [32, 64, 128],\n",
    "    'dense1_size':       [32, 64, 128, 256, 512],\n",
    "    'dropout':           uniform,\n",
    "    'learn_rate':        [0.1, 0.03, 0.001],\n",
    "    'batch_size':        [8, 16, 32, 64, 128],\n",
    "}\n",
    "\n",
    "hp_sets = ParameterSampler(hp_ranges, n_iter=2, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, hp_set in enumerate(hp_sets):\n",
    "    print()\n",
    "    print(\"Hyperparameter Set {}:\".format(i))\n",
    "    print_dict(hp_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Static hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "static_hyper_params = {\n",
    "    'activation': 'relu',\n",
    "    'conv_filter_size': 3,\n",
    "    'num_epochs': 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over `hp_sets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_valid_acc = 0.0\n",
    "best_hp_set = None\n",
    "best_hp_ind = None\n",
    "\n",
    "for hp_ind, hp_set in enumerate(hp_sets):\n",
    "    # set up wandb\n",
    "    print_header(\"Starting Training for Hyperparameter Set {}:\".format(i))\n",
    "    wandb.init()\n",
    "    ## For short runs like this, wandb.monitor()\n",
    "    # is just visual noise.  Reenable it for longer runs.\n",
    "    # wandb.monitor()\n",
    "    print_dict(hp_set)\n",
    "   \n",
    "    wandb.config.update(static_hyper_params, allow_val_change=True)\n",
    "    wandb.config.update(hp_set, allow_val_change=True)\n",
    "\n",
    "    # build model\n",
    "    model = build_compile(wandb.config)\n",
    "    print(model.summary())\n",
    "    wandb.config.num_model_parameters = model.count_params()\n",
    "    \n",
    "    # train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        epochs=wandb.config.num_epochs,\n",
    "        verbose=1,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        callbacks=[WandbCallback()]\n",
    "    )\n",
    "    \n",
    "    # track best model so far\n",
    "    valid_acc = history.history['val_acc'][-1]\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_hp_set = hp_set\n",
    "        best_hp_ind = hp_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain Best Model on Full train+valid Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_header(\"Best Hyperparams were set {} with valid accuracy {}\".format(best_hp_ind, best_valid_acc))\n",
    "print_dict(best_hp_set)\n",
    "\n",
    "# Retrain model on combined training and validation data\n",
    "wandb.config.update(best_hp_set)\n",
    "model = build_compile(wandb.config)\n",
    "history = model.fit(\n",
    "    X_train_valid, y_train_valid,\n",
    "    batch_size=wandb.config.batch_size,\n",
    "    epochs=wandb.config.num_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[WandbCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test,\n",
    "                           batch_size=wandb.config.batch_size)\n",
    "print(\"Test loss: {}, test acc: {}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Results on WandB \n",
    "Go to https://app.wandb.ai/, then select your project name to see a summary of all your runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Gotchas\n",
    "* It's easy to accidentally explode the size of your model.  In particular you get lots of parameters when:\n",
    "  * You don't use much MaxPooling\n",
    "  * You have a large first Dense layer after you Conv layers.\n",
    "* As batch size goes up, learning rate can go up.  As batch size goes down, learning rate must go down.  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "* Create a function, `build_compile_ex1`, which can create a CNN with a variable number of convolutional and dense layers using the hyperparameter ranges below.\n",
    "  * Remember that you'll need to special case the first conv layer to set `input_shape`.\n",
    "  * The hyperparameter `num_convs_per_max_pool` chooses how many conv layers should pass between each max pooling layer. \n",
    "    * You'll probably find python's modulus division operator useful for this.  e.g.: `5 % 3 ==> 2; 6 % 3 ==> 0`\n",
    "* Use the hyperparameter sets in `hp_sets_ex1` as your hyperparameter samples.\n",
    "* The number of filters in each conv layer can be constant, the number of neurons in the dense layer should be constant.\n",
    "* Include a `Dropout` layer after each `Dense` layer.\n",
    "* Don't forget the `Flatten` layer before switching to `Dense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Legal Hyperparameter Ranges\n",
    "hp_ranges_ex1 = {\n",
    "    'num_conv_filters':       [32, 64, 128],\n",
    "    'num_conv_layers':        randint(2, 8),\n",
    "    'num_convs_per_max_pool': randint(1, 3),\n",
    "    'dense_size':             [32, 64, 128, 256, 512],\n",
    "    'num_dense_layers':       randint(1, 3),\n",
    "    'dropout':                uniform,\n",
    "    'learn_rate':             [0.1, 0.03, 0.001],\n",
    "    'batch_size':             [8, 16, 32, 64, 128],\n",
    "}\n",
    "\n",
    "hp_sets_ex1 = ParameterSampler(hp_ranges_ex1, n_iter=2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, hp_set in enumerate(hp_sets_ex1):\n",
    "    print()\n",
    "    print(\"Hyperparameter Set {}:\".format(i))\n",
    "    print_dict(hp_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your `build_compile_ex1` function in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "tags": [
     "solution",
     "empty"
    ]
   },
   "outputs": [],
   "source": [
    "def build_compile_ex1(config):\n",
    "    model = Sequential()\n",
    "\n",
    "    ######### YOUR CODE HERE #########\n",
    "        \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(lr=config.learn_rate),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "static_hyper_params = {\n",
    "    'activation': 'relu',\n",
    "    'conv_filter_size': 3,\n",
    "    'num_epochs': 2,\n",
    "}\n",
    "\n",
    "best_valid_acc = 0.0\n",
    "best_hp_set = None\n",
    "best_hp_ind = None\n",
    "\n",
    "for hp_ind, hp_set in enumerate(hp_sets_ex1):\n",
    "    # set up wandb\n",
    "    print_header(\"Starting Training for Hyperparameter Set {}:\".format(i))\n",
    "    wandb.init()\n",
    "    ## For short runs like this, wandb.monitor()\n",
    "    # is just visual noise.  Reenable it for longer runs.\n",
    "    # wandb.monitor()\n",
    "    print_dict(hp_set)\n",
    "   \n",
    "    wandb.config.update(static_hyper_params, allow_val_change=True)\n",
    "    wandb.config.update(hp_set, allow_val_change=True)\n",
    "\n",
    "    # build model\n",
    "    model = build_compile_ex1(wandb.config)\n",
    "    print(model.summary())\n",
    "    wandb.config.num_model_parameters = model.count_params()\n",
    "    \n",
    "    # train model \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=wandb.config.batch_size,\n",
    "        epochs=wandb.config.num_epochs,\n",
    "        verbose=1,\n",
    "        validation_data=(X_valid, y_valid),\n",
    "        callbacks=[WandbCallback()]\n",
    "    )\n",
    "    \n",
    "    valid_acc = history.history['val_acc'][-1]\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_hp_set = hp_set\n",
    "        best_hp_ind = hp_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_header(\"Best Hyperparams were set {} with valid accuracy {}\".format(best_hp_ind, best_valid_acc))\n",
    "print_dict(best_hp_set)\n",
    "\n",
    "# Retrain model on combined training and validation data\n",
    "wandb.config.update(best_hp_set)\n",
    "model = build_compile_ex1(wandb.config)\n",
    "history = model.fit(\n",
    "    X_train_valid, y_train_valid,\n",
    "    batch_size=wandb.config.batch_size,\n",
    "    epochs=wandb.config.num_epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[WandbCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test, batch_size=wandb.config.batch_size)\n",
    "print(\"Test loss: {}, test acc: {}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "* In practice, you don't conduct a hyperparameter search by wrapping many training runs in a for loop on a single machine.  \n",
    "* Instead, you want to have a single machine which selects the hyperparameter sets, then sends them off to worker nodes which actually conduct the training.\n",
    "* Multi-node training isn't hard to do, but it's out of scope for this 1-week class; too many IT hurdles.  In this exercise, though, we'll refactor our existing code to more closely approximate a real training setup.\n",
    "\n",
    "### Instructions\n",
    "* Refactor your existing code into a script rather than a notebook.\n",
    "* The script should accept a series of keyword arguments containing all the hyperparameter values for a single run.  Check out the `argparse` python module.\n",
    "* It should then combine these arguments into a Python dict representing a single hyperparameter set like the `hp_set` variable above.\n",
    "* The script should then update the wandb.config object with the values from the input hyperparameter set and train a model using those values.  You don't need to save the final result anywhere, the `WandbCallback()` will take care of that for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "*  Create a large number of hyperparameter sets.\n",
    "*  For each hyperparameter set, print out the model summary and study the number of parameters that are produced.  Try to get a sense for what configurations produce large parameter counts.\n",
    "*  If you have time, train models based on some of these hyperparameter sets and see which produce good results and which don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
